{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the requried packages"
      ],
      "metadata": {
        "id": "2wcwqDHA04WH"
      },
      "id": "2wcwqDHA04WH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
        "\n",
        "# Install Gemma release branch from Hugging Face\n",
        "%pip install \"transformers>=4.51.3\"\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.15.2\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  \"pillow==11.1.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a4MXBjs1dd12"
      },
      "id": "a4MXBjs1dd12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the cluster to the google drive"
      ],
      "metadata": {
        "id": "jbwho5BC26Ff"
      },
      "id": "jbwho5BC26Ff"
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the notebook on to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the working directory to dl_project_fall_2025\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\")\n",
        "\n",
        "# Auto relaod doesnt work in google colab, so you can use reload to reload your function calls\n",
        "from importlib import reload"
      ],
      "metadata": {
        "id": "ZD1bIPc-eOa3"
      },
      "id": "ZD1bIPc-eOa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding PAT (Personal Access Tokens) to both Hugging Face and Google Drive"
      ],
      "metadata": {
        "id": "FYgnpney3U2b"
      },
      "id": "FYgnpney3U2b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84134828"
      },
      "source": [
        "# Importing git token and huggig face tokens\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Retrieve the GitHub Token from Colab secrets\n",
        "GH_TOKEN = userdata.get('git_token') # Ensure you stored your PAT under the secret name 'GH_TOKEN'\n",
        "hf_token = userdata.get('hugging_face')\n",
        "login(hf_token)\n",
        "\n",
        "# Configure Git to use the PAT directly in the remote URL for the 'origin'\n",
        "!git remote set-url origin https://{GH_TOKEN}@github.com/7yashwanth7/dl_project_fall_2025.git\n",
        "!git config --global user.email \"7yashwanth7@gmail.com\" # Modify to your username and pwd\n",
        "!git config --global user.name \"7yashwanth7\""
      ],
      "id": "84134828",
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the functions"
      ],
      "metadata": {
        "id": "KcjebSqM3ge3"
      },
      "id": "KcjebSqM3ge3"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "from src.llmft.data_preprocessing import preprocess\n",
        "from src.llmft.data_preprocessing import preprocess_utils"
      ],
      "metadata": {
        "id": "iW7bJOq4EBPd",
        "collapsed": true
      },
      "id": "iW7bJOq4EBPd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Models Configurations and Dataset"
      ],
      "metadata": {
        "id": "jxaT6uYl3kL7"
      },
      "id": "jxaT6uYl3kL7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model_config and json files\n",
        "defaults = preprocess_utils.read_yaml('src/llmft/config/defaults.yaml')\n",
        "cui_mapping_json = preprocess_utils.read_json('mapping_files/cui_mapping.json')\n",
        "cui_mapping = preprocess_utils.get_cui_mapping(cui_mapping_json)\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"eltorio/ROCOv2-radiology\", split=\"test\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i8PuqI9Dz64O"
      },
      "id": "i8PuqI9Dz64O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Process the datset"
      ],
      "metadata": {
        "id": "75pMrCVm3pIG"
      },
      "id": "75pMrCVm3pIG"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Image as HFImage\n",
        "dataset = dataset.cast_column(\"image\", HFImage(decode=False))\n",
        "\n",
        "# Processed Dataset\n",
        "processed_ds = dataset.map(\n",
        "    lambda b: preprocess.format_batch(b, cui_mapping, defaults),\n",
        "    batched=True,\n",
        "    batch_size=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "vb59Wjds30VN"
      },
      "id": "vb59Wjds30VN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "Rbjq66Dv9MiA"
      },
      "id": "Rbjq66Dv9MiA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-4b-pt\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n",
        "# Check if GPU benefits from bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "# Define model init arguments\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
        "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
        "    device_map=\"auto\", # Let torch decide how to load the model\n",
        ")\n",
        "# BitsAndBytesConfig int-4 config\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "# Load model and processor\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
      ],
      "metadata": {
        "id": "noyzU11cISp3",
        "collapsed": true
      },
      "id": "noyzU11cISp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Special CUI Tokens\n",
        "tokenizer = processor.tokenizer\n",
        "cui_tokens = [f\"<{cui}>\" for cui in cui_mapping.keys()]\n",
        "num_added = tokenizer.add_tokens(cui_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "processor.tokenizer = tokenizer"
      ],
      "metadata": {
        "id": "hWezxJbYHwnO"
      },
      "id": "hWezxJbYHwnO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "A6Xd5g9tQfsb"
      },
      "id": "A6Xd5g9tQfsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as PILImage\n",
        "import io\n",
        "\n",
        "def load_pil(img):\n",
        "    \"\"\"\n",
        "    Handles:\n",
        "    - HF Image with decode=False -> dict with {path, bytes}\n",
        "    - Already-decoded PIL images\n",
        "    \"\"\"\n",
        "    if isinstance(img, dict):\n",
        "        if img.get(\"bytes\") is not None:\n",
        "            return PILImage.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
        "        if img.get(\"path\"):\n",
        "            return PILImage.open(img[\"path\"]).convert(\"RGB\")\n",
        "        raise ValueError(\"Image dict missing both 'bytes' and 'path'.\")\n",
        "\n",
        "    if hasattr(img, \"convert\"):\n",
        "        return img.convert(\"RGB\")\n",
        "\n",
        "    raise ValueError(f\"Unsupported image type: {type(img)}\")\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    images = []\n",
        "\n",
        "    for example in examples:\n",
        "        # Build text from messages (which contains the image placeholder)\n",
        "        text = processor.apply_chat_template(\n",
        "            example[\"messages\"],\n",
        "            add_generation_prompt=False,\n",
        "            tokenize=False\n",
        "        )\n",
        "        texts.append(text.strip())\n",
        "\n",
        "        # IMPORTANT: pull real image from the column, not from messages\n",
        "        images.append(load_pil(example[\"image\"]))\n",
        "\n",
        "    # Tokenize & process images\n",
        "    batch = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Labels\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask padding\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    if pad_id is not None:\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "    # Mask image special tokens (safer handling)\n",
        "    boi_token = processor.tokenizer.special_tokens_map.get(\"boi_token\", None)\n",
        "    if boi_token is not None:\n",
        "        boi_id = processor.tokenizer.convert_tokens_to_ids(boi_token)\n",
        "        labels[labels == boi_id] = -100\n",
        "\n",
        "    # Keep your known extra image token id mask (if it is correct in your setup)\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "gO3yBiVL_9I2"
      },
      "id": "gO3yBiVL_9I2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "args = SFTConfig(\n",
        "    output_dir=\"gemma-product-description\",     # directory to save and repository id\n",
        "    num_train_epochs=1,                         # number of training epochs\n",
        "    per_device_train_batch_size=1,              # batch size per device during training\n",
        "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=False,                # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
        "    logging_steps=50,                            # log every 5 steps\n",
        "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
        "    bf16=True,                                  # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
        "    push_to_hub=True,                           # push model to hub\n",
        "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False\n",
        "    },  # use reentrant checkpointing\n",
        "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
        ")\n",
        "args.remove_unused_columns = False # important for collator"
      ],
      "metadata": {
        "id": "-706X_xBQmWd"
      },
      "id": "-706X_xBQmWd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "from trl import SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=processed_ds,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "# Start training, the model will be automatically saved to the Hub and the output directory\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "F364ZWZpRF1O"
      },
      "id": "F364ZWZpRF1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "JT57X8x7T1sk"
      },
      "id": "JT57X8x7T1sk"
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gemma-3-cui-finetuned-sample1\")  # saves into this directory"
      ],
      "metadata": {
        "id": "-dy5wsu5RUJZ"
      },
      "id": "-dy5wsu5RUJZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free the memory again\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DmKg87wAR7_I"
      },
      "id": "DmKg87wAR7_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_dir=\"gemma-3-cui-finetuned\""
      ],
      "metadata": {
        "id": "h2yUkH42SJZy"
      },
      "id": "h2yUkH42SJZy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "base_model_id = \"google/gemma-3-4b-pt\"\n",
        "processor_id  = \"google/gemma-3-4b-it\"          # <-- IMPORTANT: same as training\n",
        "adapter_dir   = \"gemma-3-cui-finetuned-sample1\"\n",
        "\n",
        "# 1) Load processor (IT) and tokenizer\n",
        "processor = AutoProcessor.from_pretrained(processor_id)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# 2) Re-add CUI tokens exactly like during training\n",
        "#    (you used plain codes like \"C0041618\", not \"<C0041618>\")\n",
        "cui_tokens = list(cui_mapping.keys())\n",
        "num_added = tokenizer.add_tokens(cui_tokens)\n",
        "print(\"Added\", num_added, \"CUI tokens\")\n",
        "\n",
        "processor.tokenizer = tokenizer\n",
        "\n",
        "# 3) Load base model (PT) with 4-bit quant\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_storage=torch.bfloat16,\n",
        "    ),\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    base_model_id,\n",
        "    **model_kwargs,\n",
        ")\n",
        "\n",
        "# 4) Resize embeddings to match tokenizer (base vocab + CUI tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 5) Attach LoRA adapter you trained\n",
        "model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "model.eval()"
      ],
      "metadata": {
        "collapsed": true,
        "id": "upeWEJr_TArB"
      },
      "id": "upeWEJr_TArB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_generate = dataset[1001]\n",
        "image = sample_generate[\"image\"]"
      ],
      "metadata": {
        "id": "tsRflqNZUbu7"
      },
      "id": "tsRflqNZUbu7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "system_message = \"You are a digital radiologist who can understand the medical scan of images code the concepts and provide captions\"\n",
        "\n",
        "user_prompt = \"\"\"Create a description based on the provided image and return the description of the image with details of the scan as captions, the concepts and their descriptions, only the concepts that are extracted\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": system_message},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": user_prompt},\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "        ],\n",
        "    },\n",
        "]\n"
      ],
      "metadata": {
        "id": "C2Pzyh7EVeKm"
      },
      "id": "C2Pzyh7EVeKm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn chat messages into a single string prompt\n",
        "chat_text = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,   # <-- important for inference\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# Build model inputs (batch size 1)\n",
        "inputs = processor(\n",
        "    text=[chat_text],\n",
        "    images=[image],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "# Move to correct device\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "# Option 1: decode only the new tokens (without re-printing the prompt)\n",
        "gen_only_ids = generated_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
        "output_text = tokenizer.decode(gen_only_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"MODEL OUTPUT:\\n\")\n",
        "print(output_text)"
      ],
      "metadata": {
        "id": "JQ3wb7PLVnzO"
      },
      "id": "JQ3wb7PLVnzO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1001]"
      ],
      "metadata": {
        "id": "X7W10Cy6VyoH"
      },
      "id": "X7W10Cy6VyoH",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}