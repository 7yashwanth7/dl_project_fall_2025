{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install the requried packages"
      ],
      "metadata": {
        "id": "2wcwqDHA04WH"
      },
      "id": "2wcwqDHA04WH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Pytorch & other libraries\n",
        "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
        "\n",
        "# Install Gemma release branch from Hugging Face\n",
        "%pip install \"transformers>=4.51.3\"\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.15.2\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  \"pillow==11.1.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a4MXBjs1dd12"
      },
      "id": "a4MXBjs1dd12",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount the cluster to the google drive"
      ],
      "metadata": {
        "id": "jbwho5BC26Ff"
      },
      "id": "jbwho5BC26Ff"
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the notebook on to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the working directory to dl_project_fall_2025\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\")\n",
        "\n",
        "# Auto relaod doesnt work in google colab, so you can use reload to reload your function calls\n",
        "from importlib import reload"
      ],
      "metadata": {
        "id": "ZD1bIPc-eOa3"
      },
      "id": "ZD1bIPc-eOa3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Adding PAT (Personal Access Tokens) to both Hugging Face and Google Drive"
      ],
      "metadata": {
        "id": "FYgnpney3U2b"
      },
      "id": "FYgnpney3U2b"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84134828"
      },
      "source": [
        "# Importing git token and huggig face tokens\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Retrieve the GitHub Token from Colab secrets\n",
        "GH_TOKEN = userdata.get('git_token') # Ensure you stored your PAT under the secret name 'GH_TOKEN'\n",
        "hf_token = userdata.get('hugging_face')\n",
        "login(hf_token)\n",
        "\n",
        "# Configure Git to use the PAT directly in the remote URL for the 'origin'\n",
        "!git remote set-url origin https://{GH_TOKEN}@github.com/7yashwanth7/dl_project_fall_2025.git\n",
        "!git config --global user.email \"7yashwanth7@gmail.com\" # Modify to your username and pwd\n",
        "!git config --global user.name \"7yashwanth7\""
      ],
      "id": "84134828",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the functions"
      ],
      "metadata": {
        "id": "KcjebSqM3ge3"
      },
      "id": "KcjebSqM3ge3"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n",
        "from datasets import load_dataset\n",
        "from PIL import Image\n",
        "\n",
        "from src.llmft.data_preprocessing import preprocess\n",
        "from src.llmft.data_preprocessing import preprocess_utils"
      ],
      "metadata": {
        "id": "iW7bJOq4EBPd",
        "collapsed": true
      },
      "id": "iW7bJOq4EBPd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load Models Configurations and Dataset"
      ],
      "metadata": {
        "id": "jxaT6uYl3kL7"
      },
      "id": "jxaT6uYl3kL7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model_config and json files\n",
        "defaults = preprocess_utils.read_yaml('src/llmft/config/defaults.yaml')\n",
        "cui_mapping_json = preprocess_utils.read_json('mapping_files/cui_mapping.json')\n",
        "cui_mapping = preprocess_utils.get_cui_mapping(cui_mapping_json)\n",
        "\n",
        "# Load dataset from the hub\n",
        "dataset = load_dataset(\"eltorio/ROCOv2-radiology\", split=\"validation\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "i8PuqI9Dz64O"
      },
      "id": "i8PuqI9Dz64O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pre-Process the datset"
      ],
      "metadata": {
        "id": "75pMrCVm3pIG"
      },
      "id": "75pMrCVm3pIG"
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Image as HFImage\n",
        "dataset = dataset.cast_column(\"image\", HFImage(decode=False))\n",
        "\n",
        "# Processed Dataset\n",
        "processed_ds = dataset.map(\n",
        "    lambda b: preprocess.format_batch(b, cui_mapping, defaults),\n",
        "    batched=True,\n",
        "    batch_size=1024,\n",
        ")"
      ],
      "metadata": {
        "id": "vb59Wjds30VN"
      },
      "id": "vb59Wjds30VN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ],
      "metadata": {
        "id": "Rbjq66Dv9MiA"
      },
      "id": "Rbjq66Dv9MiA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face model id\n",
        "model_id = \"google/gemma-3-4b-pt\" # or `google/gemma-3-12b-pt`, `google/gemma-3-27-pt`\n",
        "# Check if GPU benefits from bfloat16\n",
        "if torch.cuda.get_device_capability()[0] < 8:\n",
        "    raise ValueError(\"GPU does not support bfloat16, please use a GPU that supports bfloat16.\")\n",
        "# Define model init arguments\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\", # Use \"flash_attention_2\" when running on Ampere or newer GPU\n",
        "    torch_dtype=torch.bfloat16, # What torch dtype to use, defaults to auto\n",
        "    device_map=\"auto\", # Let torch decide how to load the model\n",
        ")\n",
        "# BitsAndBytesConfig int-4 config\n",
        "model_kwargs[\"quantization_config\"] = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=model_kwargs[\"torch_dtype\"],\n",
        "    bnb_4bit_quant_storage=model_kwargs[\"torch_dtype\"],\n",
        ")\n",
        "# Load model and processor\n",
        "model = AutoModelForImageTextToText.from_pretrained(model_id, **model_kwargs)\n",
        "processor = AutoProcessor.from_pretrained(\"google/gemma-3-4b-it\")"
      ],
      "metadata": {
        "id": "noyzU11cISp3",
        "collapsed": true
      },
      "id": "noyzU11cISp3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding Special CUI Tokens\n",
        "tokenizer = processor.tokenizer\n",
        "cui_tokens = [f\"<{cui}>\" for cui in cui_mapping.keys()]\n",
        "num_added = tokenizer.add_tokens(cui_tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "processor.tokenizer = tokenizer"
      ],
      "metadata": {
        "id": "hWezxJbYHwnO"
      },
      "id": "hWezxJbYHwnO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig\n",
        "peft_config = LoraConfig(\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    r=16,\n",
        "    bias=\"none\",\n",
        "    target_modules=\"all-linear\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    modules_to_save=[\n",
        "        \"lm_head\",\n",
        "        \"embed_tokens\",\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "A6Xd5g9tQfsb"
      },
      "id": "A6Xd5g9tQfsb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image as PILImage\n",
        "import io\n",
        "\n",
        "def load_pil(img):\n",
        "    \"\"\"\n",
        "    Handles:\n",
        "    - HF Image with decode=False -> dict with {path, bytes}\n",
        "    - Already-decoded PIL images\n",
        "    \"\"\"\n",
        "    if isinstance(img, dict):\n",
        "        if img.get(\"bytes\") is not None:\n",
        "            return PILImage.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
        "        if img.get(\"path\"):\n",
        "            return PILImage.open(img[\"path\"]).convert(\"RGB\")\n",
        "        raise ValueError(\"Image dict missing both 'bytes' and 'path'.\")\n",
        "\n",
        "    if hasattr(img, \"convert\"):\n",
        "        return img.convert(\"RGB\")\n",
        "\n",
        "    raise ValueError(f\"Unsupported image type: {type(img)}\")\n",
        "\n",
        "\n",
        "def collate_fn(examples):\n",
        "    texts = []\n",
        "    images = []\n",
        "\n",
        "    for example in examples:\n",
        "        # Build text from messages (which contains the image placeholder)\n",
        "        text = processor.apply_chat_template(\n",
        "            example[\"messages\"],\n",
        "            add_generation_prompt=False,\n",
        "            tokenize=False\n",
        "        )\n",
        "        texts.append(text.strip())\n",
        "\n",
        "        # IMPORTANT: pull real image from the column, not from messages\n",
        "        images.append(load_pil(example[\"image\"]))\n",
        "\n",
        "    # Tokenize & process images\n",
        "    batch = processor(\n",
        "        text=texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True\n",
        "    )\n",
        "\n",
        "    # Labels\n",
        "    labels = batch[\"input_ids\"].clone()\n",
        "\n",
        "    # Mask padding\n",
        "    pad_id = processor.tokenizer.pad_token_id\n",
        "    if pad_id is not None:\n",
        "        labels[labels == pad_id] = -100\n",
        "\n",
        "    # Mask image special tokens (safer handling)\n",
        "    boi_token = processor.tokenizer.special_tokens_map.get(\"boi_token\", None)\n",
        "    if boi_token is not None:\n",
        "        boi_id = processor.tokenizer.convert_tokens_to_ids(boi_token)\n",
        "        labels[labels == boi_id] = -100\n",
        "\n",
        "    # Keep your known extra image token id mask (if it is correct in your setup)\n",
        "    labels[labels == 262144] = -100\n",
        "\n",
        "    batch[\"labels\"] = labels\n",
        "    return batch"
      ],
      "metadata": {
        "id": "gO3yBiVL_9I2"
      },
      "id": "gO3yBiVL_9I2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTConfig\n",
        "args = SFTConfig(\n",
        "    output_dir=\"gemma-product-description\",     # directory to save and repository id\n",
        "    num_train_epochs=1,                         # number of training epochs\n",
        "    per_device_train_batch_size=1,              # batch size per device during training\n",
        "    gradient_accumulation_steps=4,              # number of steps before performing a backward/update pass\n",
        "    gradient_checkpointing=True,                # use gradient checkpointing to save memory\n",
        "    optim=\"adamw_torch_fused\",                  # use fused adamw optimizer\n",
        "    logging_steps=5,                            # log every 5 steps\n",
        "    save_strategy=\"epoch\",                      # save checkpoint every epoch\n",
        "    learning_rate=2e-4,                         # learning rate, based on QLoRA paper\n",
        "    bf16=True,                                  # use bfloat16 precision\n",
        "    max_grad_norm=0.3,                          # max gradient norm based on QLoRA paper\n",
        "    warmup_ratio=0.03,                          # warmup ratio based on QLoRA paper\n",
        "    lr_scheduler_type=\"constant\",               # use constant learning rate scheduler\n",
        "    push_to_hub=True,                           # push model to hub\n",
        "    report_to=\"tensorboard\",                    # report metrics to tensorboard\n",
        "    gradient_checkpointing_kwargs={\n",
        "        \"use_reentrant\": False\n",
        "    },  # use reentrant checkpointing\n",
        "    dataset_text_field=\"\",                      # need a dummy field for collator\n",
        "    dataset_kwargs={\"skip_prepare_dataset\": True},  # important for collator\n",
        ")\n",
        "args.remove_unused_columns = False # important for collator"
      ],
      "metadata": {
        "id": "j87_H0hmR6TB"
      },
      "id": "j87_H0hmR6TB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Default title text\n",
        "from trl import SFTTrainer\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=processed_ds,\n",
        "    peft_config=peft_config,\n",
        "    processing_class=processor,\n",
        "    data_collator=collate_fn,\n",
        ")\n",
        "# Start training, the model will be automatically saved to the Hub and the output directory\n",
        "trainer.train()\n",
        "trainer.save_model(\"gemma-3-cui-finetuned-sample1\")  # saves into this directory"
      ],
      "metadata": {
        "id": "F364ZWZpRF1O",
        "collapsed": true
      },
      "id": "F364ZWZpRF1O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# free the memory again\n",
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "DmKg87wAR7_I"
      },
      "id": "DmKg87wAR7_I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# disconnect Colab runtime\n",
        "try:\n",
        "    from google.colab import runtime\n",
        "    runtime.unassign()\n",
        "except Exception as e:\n",
        "    print(\"Could not unassign runtime (maybe not in Colab):\", e)"
      ],
      "metadata": {
        "id": "lTK1tHjMRV4h"
      },
      "id": "lTK1tHjMRV4h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qU4Eb3meXY48"
      },
      "id": "qU4Eb3meXY48",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}