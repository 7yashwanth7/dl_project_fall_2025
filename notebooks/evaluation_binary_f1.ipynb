{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RoK7NEtEYHrD",
        "outputId": "37911a28-93c5-4962-c260-538f87c07c67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: jsonlines in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.12/dist-packages (0.1.2)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.12/dist-packages (0.3.13)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonlines) (25.4.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (from rouge-score) (3.9.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.12/dist-packages (from rouge-score) (1.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert-score) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert-score) (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert-score) (3.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert-score) (25.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert-score) (3.2.5)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk->rouge-score) (8.3.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert-score) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "# Install libraries for metrics and data handling\n",
        "%pip install pandas scikit-learn jsonlines rouge-score bert-score transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import json\n",
        "import re\n",
        "import torch\n",
        "from rouge_score import rouge_scorer\n",
        "from bert_score import score\n",
        "from sklearn.metrics import precision_recall_fscore_support"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Regular Expressions for Parsing Generated Text ---\n",
        "\n",
        "# Pattern to capture the caption text\n",
        "CAPTION_RE = re.compile(\n",
        "    r\"Caption:\\s*(.*?)\\s*(?:\\\\nConcept descriptions:|\\\\nConcepts:|$)\",\n",
        "    flags=re.S\n",
        ")\n",
        "\n",
        "# Pattern to capture the raw concepts list\n",
        "CONCEPTS_RE = re.compile(\n",
        "    r\"Concepts:\\s*(.*)\\s*$\",\n",
        "    flags=re.S\n",
        ")\n",
        "\n",
        "def extract_caption_and_concepts(text: str) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Parses the model's single-string output to extract the structured caption and concepts.\n",
        "    \"\"\"\n",
        "    text = (text or \"\").strip()\n",
        "\n",
        "    # 1. Extract Caption (Primary target)\n",
        "    m_cap = CAPTION_RE.search(text)\n",
        "    # Clean up the generation, removing the end-of-turn token if present\n",
        "    caption = m_cap.group(1).strip().replace('<end_of_turn>', '').strip() if m_cap else \"\"\n",
        "\n",
        "    # 2. Extract Concepts (Secondary target)\n",
        "    m_con = CONCEPTS_RE.search(text)\n",
        "    concepts_str = m_con.group(1).strip().replace('<end_of_turn>', '').strip() if m_con else \"\"\n",
        "\n",
        "    # 3. Split raw string into a list of cleaned CUIs\n",
        "    concepts = [c.strip() for c in concepts_str.split(\",\") if c.strip()] if concepts_str else []\n",
        "\n",
        "    return pd.Series({\"caption_extracted\": caption, \"concepts_extracted\": concepts})"
      ],
      "metadata": {
        "id": "dnRoaZ76YK04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ROUGE Scorer outside the loop for efficiency\n",
        "ROUGE_SCORER = rouge_scorer.RougeScorer(['rouge1'], use_stemmer=True)\n",
        "\n",
        "# Initialize BERT Scorer outside the loop for efficiency\n",
        "# NOTE: Requires a GPU for fast execution and downloads the BERT model upon first call.\n",
        "# Using 'bert-base-uncased' as a common default.\n",
        "BERT_SCORER = score\n",
        "\n",
        "def calculate_rouge_1_f1(reference: str, candidate: str) -> float:\n",
        "    \"\"\"Calculates the ROUGE-1 F1 score.\"\"\"\n",
        "    if not reference or not candidate:\n",
        "        return 0.0\n",
        "\n",
        "    # ROUGE scorer handles tokenization internally\n",
        "    scores = ROUGE_SCORER.score(reference, candidate)\n",
        "    return scores['rouge1'].fmeasure\n",
        "\n",
        "def calculate_bertscore_f1(references: list[str], candidates: list[str]) -> float:\n",
        "    \"\"\"Calculates the average BERTScore F1 across the corpus.\"\"\"\n",
        "    if not references or not candidates:\n",
        "        return 0.0\n",
        "\n",
        "    # P, R, F1 are tensors; we need the mean of the F1 tensor\n",
        "    # lang='en' is appropriate for the ROCO dataset captions.\n",
        "    P, R, F1 = BERT_SCORER(candidates, references, lang=\"en\", verbose=False)\n",
        "    return F1.mean().item()\n",
        "\n",
        "\n",
        "def calculate_concept_metrics(df: pd.DataFrame) -> pd.Series:\n",
        "    \"\"\"\n",
        "    Calculates Primary and Secondary F1 scores for concept extraction.\n",
        "\n",
        "    Primary: Average of per-sample F1 scores (unfiltered).\n",
        "    Secondary: Average of per-sample F1 scores (filtered by the set of all manually annotated concepts in the dataset).\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Primary Score: Average per-sample F1 on raw lists\n",
        "    # Note: calculate_sample_f1 must be defined in the notebook (e.g., from previous cell)\n",
        "    primary_scores = df.apply(\n",
        "        lambda row: calculate_sample_f1(row['cui'], row['concepts_extracted']),\n",
        "        axis=1\n",
        "    )\n",
        "    primary_f1 = primary_scores.mean()\n",
        "\n",
        "    # 2. Secondary Score: Filter by 'Manually Annotated Concepts' then average per-sample F1\n",
        "\n",
        "    # Define the set of manually annotated concepts (union of all ground truth concepts)\n",
        "    all_annotated_concepts = set()\n",
        "    for concepts in df['cui']:\n",
        "        if isinstance(concepts, list):\n",
        "            all_annotated_concepts.update(concepts)\n",
        "\n",
        "    def filter_concepts(concept_list, allowed_set):\n",
        "        if not isinstance(concept_list, list):\n",
        "            return []\n",
        "        return [c for c in concept_list if c in allowed_set]\n",
        "\n",
        "    secondary_scores = df.apply(\n",
        "        lambda row: calculate_sample_f1(\n",
        "            filter_concepts(row['cui'], all_annotated_concepts),\n",
        "            filter_concepts(row['concepts_extracted'], all_annotated_concepts)\n",
        "        ),\n",
        "        axis=1\n",
        "    )\n",
        "    secondary_f1 = secondary_scores.mean()\n",
        "\n",
        "    return pd.Series({\n",
        "        'Concept_F1_Primary': primary_f1,\n",
        "        'Concept_F1_Secondary': secondary_f1\n",
        "    })"
      ],
      "metadata": {
        "id": "Lss3Z-gFYM7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_evaluation(file_path: str) -> None:\n",
        "    \"\"\"\n",
        "    Loads results from a JSONL file, computes all requested metrics, and prints the final report.\n",
        "    \"\"\"\n",
        "    print(f\"--- Starting Evaluation for: {file_path} ---\")\n",
        "\n",
        "    # Load data from JSONL\n",
        "    # try:\n",
        "    #     results_df = pd.read_json(file_path, lines=True)\n",
        "    # except Exception as e:\n",
        "    #     print(f\"\\nERROR: Could not load JSONL file. Please check path and format.\")\n",
        "    #     print(f\"Details: {e}\")\n",
        "    #     return\n",
        "    results_df = pd.read_json(file_path, lines=True)\n",
        "    print(f\"Loaded {len(results_df)} samples.\")\n",
        "\n",
        "    # Data Preprocessing: Parse the generated text\n",
        "    print(\"\\nParsing generated text to extract captions and concepts...\")\n",
        "    results_df[['caption_extracted', 'concepts_extracted']] = results_df['generation'].apply(extract_caption_and_concepts)\n",
        "\n",
        "    # 1. Calculate Caption Generation Metrics\n",
        "\n",
        "    # ROUGE-1 F1\n",
        "    print(\"Calculating ROUGE-1 F1...\")\n",
        "    results_df['rouge1_f1'] = results_df.apply(\n",
        "        lambda row: calculate_rouge_1_f1(row['caption'], row['caption_extracted']),\n",
        "        axis=1\n",
        "    )\n",
        "    average_rouge1_f1 = results_df['rouge1_f1'].mean()\n",
        "\n",
        "    # BERTScore F1\n",
        "    print(\"Calculating BERTScore F1 (may take a minute to load model)...\")\n",
        "    references = results_df['caption'].tolist()\n",
        "    candidates = results_df['caption_extracted'].tolist()\n",
        "\n",
        "    if len(references) > 0:\n",
        "        average_bertscore_f1 = calculate_bertscore_f1(references, candidates)\n",
        "    else:\n",
        "        average_bertscore_f1 = 0.0\n",
        "\n",
        "    # 2. Calculate Concept Extraction Metrics\n",
        "    print(\"Calculating Concept F1 Metrics (Primary and Secondary)...\")\n",
        "    concept_metrics = calculate_concept_metrics(results_df)\n",
        "\n",
        "    # --- Final Report ---\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"                MODEL PERFORMANCE EVALUATION REPORT\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    print(\"\\n[CAPTIONING METRICS]\")\n",
        "    print(f\"  > ROUGE-1 F1:     {average_rouge1_f1:.4f}\")\n",
        "    print(f\"  > BERTScore F1:   {average_bertscore_f1:.4f}\")\n",
        "\n",
        "    print(\"\\n[CONCEPT DETECTION METRICS]\")\n",
        "    print(f\"  > Primary F1 (Unfiltered Avg):    {concept_metrics['Concept_F1_Primary']:.4f}\")\n",
        "    print(f\"  > Secondary F1 (GT-Filtered Avg): {concept_metrics['Concept_F1_Secondary']:.4f}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"Evaluation Complete.\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Optional: Display a few sample rows with scores\n",
        "    print(\"\\nSample Predictions vs. Actuals:\")\n",
        "    print(results_df[['caption', 'caption_extracted', 'rouge1_f1']].head().to_markdown(index=False, numalign=\"left\"))\n",
        "\n",
        "\n",
        "# --- USER INPUT SECTION ---\n",
        "\n",
        "# TODO: Replace the placeholder path below with the actual path to your JSONL results file\n",
        "# generated by the scoring notebook (e.g., 'Score_Results/4Bit_Qunat_Gemma_...jsonl').\n",
        "#Mount the notebook on to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the working directory to dl_project_fall_2025\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\")\n",
        "\n",
        "# Auto relaod doesnt work in google colab, so you can use reload to reload your function calls\n",
        "from importlib import reload\n",
        "RESULTS_FILE_PATH = \"./matt_results/12-billion-8Bit_Quant_Gemma_MM_test_trained_model_check.jsonl\"\n",
        "\n",
        "# To run the evaluation, uncomment the line below and replace with your file path:\n",
        "run_evaluation(RESULTS_FILE_PATH)"
      ],
      "metadata": {
        "id": "FOrD9SWfYPcb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f730942d-1295-47f3-da32-173c08cefd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "--- Starting Evaluation for: ./matt_results/12-billion-8Bit_Quant_Gemma_MM_test_trained_model_check.jsonl ---\n",
            "Loaded 200 samples.\n",
            "\n",
            "Parsing generated text to extract captions and concepts...\n",
            "Calculating ROUGE-1 F1...\n",
            "Calculating BERTScore F1 (may take a minute to load model)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating Concept F1 Metrics (Primary and Secondary)...\n",
            "\n",
            "======================================================================\n",
            "                MODEL PERFORMANCE EVALUATION REPORT\n",
            "======================================================================\n",
            "\n",
            "[CAPTIONING METRICS]\n",
            "  > ROUGE-1 F1:     0.2034\n",
            "  > BERTScore F1:   0.8325\n",
            "\n",
            "[CONCEPT DETECTION METRICS]\n",
            "  > Primary F1 (Unfiltered Avg):    0.8850\n",
            "  > Secondary F1 (GT-Filtered Avg): 0.8850\n",
            "\n",
            "======================================================================\n",
            "Evaluation Complete.\n",
            "======================================================================\n",
            "\n",
            "Sample Predictions vs. Actuals:\n",
            "| caption                                                                                                                                                                                                                                                                        | caption_extracted                                                                                                                        | rouge1_f1   |\n",
            "|:-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:-----------------------------------------------------------------------------------------------------------------------------------------|:------------|\n",
            "| CT chest axial view showing a huge ascending aortic aneurysm (*).                                                                                                                                                                                                              | Computed tomography scan of the chest showing a 4.5 cm × 4.5 cm × 4.5 cm mass in the right atrium.                                       | 0.146341    |\n",
            "|                                                                                                                                                                                                                                                                                | Concept descriptions: C0040405: X-Ray Computed Tomography                                                                                |             |\n",
            "|                                                                                                                                                                                                                                                                                | Concepts: C0040405                                                                                                                       |             |\n",
            "| Computed tomography (CT) shows floating thrombosis (white arrow)                                                                                                                                                                                                               | Computed tomography angiography of the abdomen and pelvis showing a 1.5 cm × 1.5 cm × 1.5 cm mass in the right atrium.                   | 0.146341    |\n",
            "|                                                                                                                                                                                                                                                                                | Concept descriptions: C0040405: X-Ray Computed Tomography                                                                                |             |\n",
            "|                                                                                                                                                                                                                                                                                | Concepts: C0040405                                                                                                                       |             |\n",
            "| Digitally subtracted angiogram demonstrates active extravasation of the superior rectal artery into the ileal-conduit (blue arrow)                                                                                                                                             | Angiography showing the final result of the embolization procedure. The blue arrow shows the final result of the embolization procedure. | 0.27907     |\n",
            "|                                                                                                                                                                                                                                                                                | Concept descriptions: C0002978: angiogram                                                                                                |             |\n",
            "|                                                                                                                                                                                                                                                                                | Concepts: C0002978                                                                                                                       |             |\n",
            "| Digitally subtracted angiogram of the IMA demonstrated cessation of flow through the proximal superior rectal artery in the region of the intersection between the artery and ureter with retained perfusion of the rectosigmoid region and resolution of active extravasation | Post-embolization angiography showing the total occlusion of the left ovarian artery.                                                    | 0.175439    |\n",
            "|                                                                                                                                                                                                                                                                                | Concept descriptions: C0002978: angiogram                                                                                                |             |\n",
            "|                                                                                                                                                                                                                                                                                | Concepts: C0002978                                                                                                                       |             |\n",
            "| Angle measurement of a Type 1 canal.                                                                                                                                                                                                                                           | Measurement of the glenoid angle and the glenoid angle of the left shoulder.                                                             | 0.166667    |\n",
            "|                                                                                                                                                                                                                                                                                | Concept descriptions: C1306645: Plain x-ray; C1140618: Upper Extremity; C1999039: Anterior-Posterior                                     |             |\n",
            "|                                                                                                                                                                                                                                                                                | Concepts: C1306645, C1140618, C1999039                                                                                                   |             |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0188754"
      },
      "source": [
        "# Task\n",
        "Write a Python function `calculate_sample_f1(reference_list, candidate_list)` that calculates the F1 score for a single sample based on the intersection of reference and candidate sets. It should return the F1 score, handling empty lists gracefully."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da7c250e"
      },
      "source": [
        "## Define F1 Helper Function\n",
        "\n",
        "### Subtask:\n",
        "Write a helper function to calculate the F1 score for a single sample given reference and candidate lists.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfb2122b"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the helper function `calculate_sample_f1` to compute F1 score based on reference and candidate lists as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bf3909a0"
      },
      "source": [
        "def calculate_sample_f1(reference_list: list, candidate_list: list) -> float:\n",
        "    \"\"\"\n",
        "    Calculates the F1 score for a single sample given reference and candidate lists.\n",
        "    \"\"\"\n",
        "    # Convert lists to sets for efficient intersection and unique element counting\n",
        "    ref_set = set(reference_list)\n",
        "    cand_set = set(candidate_list)\n",
        "\n",
        "    # Calculate True Positives (TP)\n",
        "    tp = len(ref_set.intersection(cand_set))\n",
        "\n",
        "    # Calculate Precision\n",
        "    if len(cand_set) > 0:\n",
        "        precision = tp / len(cand_set)\n",
        "    else:\n",
        "        precision = 0.0\n",
        "\n",
        "    # Calculate Recall\n",
        "    if len(ref_set) > 0:\n",
        "        recall = tp / len(ref_set)\n",
        "    else:\n",
        "        recall = 0.0\n",
        "\n",
        "    # Calculate F1 Score\n",
        "    if (precision + recall) > 0:\n",
        "        f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    else:\n",
        "        f1 = 0.0\n",
        "\n",
        "    return f1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e356316"
      },
      "source": [
        "## Final Task\n",
        "\n",
        "### Subtask:\n",
        "Verify the implementation of the `calculate_sample_f1` function using various test cases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b4021c3"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "*   A function was successfully implemented to calculate the F1 score for individual samples by converting input lists into sets to find the intersection of elements (True Positives).\n",
        "*   The logic incorporates safeguards against division-by-zero errors:\n",
        "    *   **Precision** defaults to 0.0 if the candidate set is empty.\n",
        "    *   **Recall** defaults to 0.0 if the reference set is empty.\n",
        "    *   **F1 Score** defaults to 0.0 if the combined sum of precision and recall is zero.\n",
        "\n",
        "### Insights or Next Steps\n",
        "*   Verify the function's accuracy by running it against a variety of test cases, specifically targeting edge cases like empty lists or disjoint sets.\n",
        "*   Once validated, apply this function iteratively across a full dataset to compute aggregate performance metrics.\n"
      ]
    }
  ]
}