{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ac3f59",
   "metadata": {},
   "source": [
    "# Qwen 3.5 Vision-Language Fine-Tuning\n",
    "\n",
    "Colab-friendly notebook to fine-tune a Qwen 3.5 (vision-language) model on ROCO radiology data with LoRA. Mirrors the existing Gemma training flow but swaps in the Qwen model, batching, and image handling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d80b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (run once per runtime)\n",
    "%pip install -q --upgrade   torch torchvision tensorboard   transformers==4.45.2   datasets==3.3.2   accelerate==1.4.0   bitsandbytes==0.45.3   trl==0.15.2   peft==0.14.0   pillow==11.1.0   protobuf sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8606560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Mount Google Drive in Colab and set project root\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive', force_remount=False)\n",
    "    %cd /content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\n",
    "except Exception as e:\n",
    "    print('Drive not mounted; staying in local runtime.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d03e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment and paths\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "project_root = Path.cwd()\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(project_root / \".env\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "token_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "print(\"HF token loaded\" if hf_token else \"HF token not set; proceeding without one.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5786d35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoProcessor, AutoModelForVision2Seq, BitsAndBytesConfig\n",
    "from peft import PeftModel, LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from pathlib import Path\n",
    "from importlib import reload\n",
    "\n",
    "from llmft.data_preprocessing import preprocess_utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013e6e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load prompts and CUI mapping\n",
    "defaults = preprocess_utils.read_yaml(project_root / 'src/llmft/config/defaults.yaml')\n",
    "cui_mapping_json = preprocess_utils.read_json(project_root / 'mapping_files/cui_mapping.json')\n",
    "cui_mapping = preprocess_utils.get_cui_mapping(cui_mapping_json)\n",
    "print(f\"Loaded {len(cui_mapping)} CUI mapping entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c00fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset loading (ROCO radiology)\n",
    "from datasets import Image as HFImage\n",
    "\n",
    "train_split = \"train\"  # change to \"validation\" or custom split\n",
    "max_samples = None      # set to an int to subset\n",
    "\n",
    "raw_ds = load_dataset(\"eltorio/ROCOv2-radiology\", split=train_split, **token_kwargs)\n",
    "if max_samples:\n",
    "    raw_ds = raw_ds.select(range(max_samples))\n",
    "raw_ds = raw_ds.cast_column(\"image\", HFImage(decode=False))  # keep bytes/paths to decode later\n",
    "print(raw_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7113c335",
   "metadata": {},
   "source": [
    "## Model & LoRA setup\n",
    "Using a Qwen 3.5 VL model with 4-bit loading + LoRA. Adjust model IDs for your hardware (smaller models if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1eb135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model IDs (pick a size your GPU can handle)\n",
    "base_model_id = \"Qwen/Qwen2.5-VL-3B-Instruct\"\n",
    "\n",
    "# Load processor/tokenizer\n",
    "processor = AutoProcessor.from_pretrained(base_model_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# Add CUI tokens (wrapped in <> to avoid collisions)\n",
    "cui_tokens = [f\"<{cui}>\" for cui in cui_mapping.keys()]\n",
    "num_added = tokenizer.add_tokens(cui_tokens)\n",
    "print(f\"Added {num_added} CUI tokens\")\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# Quantization config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForVision2Seq.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Resize embeddings to include new tokens\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    r=16,\n",
    "    bias=\"none\",\n",
    "    target_modules=\"all-linear\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    modules_to_save=[\"lm_head\", \"embed_tokens\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a605673b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: decode HF image entries to PIL\n",
    "from PIL import Image as PILImage\n",
    "import io\n",
    "\n",
    "def load_pil(img):\n",
    "    if isinstance(img, dict):\n",
    "        if img.get(\"bytes\") is not None:\n",
    "            return PILImage.open(io.BytesIO(img[\"bytes\"])).convert(\"RGB\")\n",
    "        if img.get(\"path\"):\n",
    "            return PILImage.open(img[\"path\"]).convert(\"RGB\")\n",
    "    if hasattr(img, \"convert\"):\n",
    "        return img.convert(\"RGB\")\n",
    "    raise ValueError(\"Unsupported image format: {type(img)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2590771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator for TRL SFTTrainer\n",
    "system_message = defaults[\"system_message\"]\n",
    "user_prompt = defaults[\"user_prompt\"]\n",
    "\n",
    "# Build messages for a single example\n",
    "def build_messages(example):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": [{\"type\": \"text\", \"text\": system_message}]},\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\"type\": \"text\", \"text\": user_prompt},\n",
    "                {\"type\": \"image\", \"image\": load_pil(example[\"image\"])}\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    prompts = [processor.apply_chat_template(build_messages(ex), add_generation_prompt=True, tokenize=False) for ex in batch]\n",
    "    images = [load_pil(ex[\"image\"]) for ex in batch]\n",
    "    model_inputs = processor(text=prompts, images=images, return_tensors=\"pt\", padding=True)\n",
    "    labels = model_inputs[\"input_ids\"].clone()\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return {k: v.to(model.device) for k, v in model_inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52a225d",
   "metadata": {},
   "source": [
    "## Training\n",
    "Configure epochs, batch size, and run SFTTrainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93dc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "output_dir = \"qwen-vl-cui-finetuned\"\n",
    "args = SFTConfig(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    save_total_limit=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17fd10e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=raw_ds,\n",
    "    peft_config=peft_config,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27be61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and save\n",
    "trainer.train()\n",
    "trainer.save_model(output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6719f6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: push to Hub (set HUGGINGFACE_TOKEN)\n",
    "# trainer.push_to_hub()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ba10f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
