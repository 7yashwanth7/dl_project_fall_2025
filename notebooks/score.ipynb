{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de3ca81",
      "metadata": {
        "collapsed": true,
        "id": "6de3ca81"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
        "\n",
        "# Install Gemma release branch from Hugging Face\n",
        "%pip install \"transformers>=4.51.3\"\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.15.2\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  \"pillow==11.1.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the notebook on to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the working directory to dl_project_fall_2025\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\")\n",
        "\n",
        "# Auto relaod doesnt work in google colab, so you can use reload to reload your function calls\n",
        "from importlib import reload"
      ],
      "metadata": {
        "id": "kDaNMKiLfvz3"
      },
      "id": "kDaNMKiLfvz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing git token and huggig face tokens\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Retrieve the GitHub Token from Colab secrets\n",
        "GH_TOKEN = userdata.get('git_token') # Ensure you stored your PAT under the secret name 'GH_TOKEN'\n",
        "hf_token = userdata.get('hugging_face')\n",
        "login(hf_token)\n",
        "\n",
        "# Configure Git to use the PAT directly in the remote URL for the 'origin'\n",
        "!git remote set-url origin https://{GH_TOKEN}@github.com/7yashwanth7/dl_project_fall_2025.git\n",
        "!git config --global user.email \"7yashwanth7@gmail.com\" # Modify to your username and pwd\n",
        "!git config --global user.name \"7yashwanth7\""
      ],
      "metadata": {
        "id": "JVmACrOhfzJI"
      },
      "id": "JVmACrOhfzJI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86645fe",
      "metadata": {
        "id": "e86645fe"
      },
      "outputs": [],
      "source": [
        "from src.llmft.data_preprocessing import preprocess_utils\n",
        "\n",
        "defaults = preprocess_utils.read_yaml('src/llmft/config/defaults.yaml')\n",
        "cui_mapping_json = preprocess_utils.read_json('mapping_files/cui_mapping.json')\n",
        "cui_mapping = preprocess_utils.get_cui_mapping(cui_mapping_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b17dcaa",
      "metadata": {
        "id": "6b17dcaa"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "base_model_id = \"google/gemma-3-4b-pt\"\n",
        "processor_id  = \"google/gemma-3-4b-it\"          # <-- IMPORTANT: same as training\n",
        "adapter_dir   = \"gemma-3-cui-finetuned-sample1\"\n",
        "\n",
        "# 1) Load processor (IT) and tokenizer\n",
        "processor = AutoProcessor.from_pretrained(processor_id)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "# 2) Re-add CUI tokens exactly like during training\n",
        "#    (you used plain codes like \"C0041618\", not \"\")\n",
        "cui_tokens = list(cui_mapping.keys())\n",
        "num_added = tokenizer.add_tokens(cui_tokens)\n",
        "print(\"Added\", num_added, \"CUI tokens\")\n",
        "\n",
        "processor.tokenizer = tokenizer\n",
        "\n",
        "# 3) Load base model (PT) with 4-bit quant\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_storage=torch.bfloat16,\n",
        "    ),\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    base_model_id,\n",
        "    **model_kwargs,\n",
        ")\n",
        "\n",
        "# 4) Resize embeddings to match tokenizer (base vocab + CUI tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 5) Attach LoRA adapter you trained\n",
        "model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc555f3",
      "metadata": {
        "collapsed": true,
        "id": "2fc555f3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_specs = [\n",
        "    {\"name\": \"eltorio/ROCOv2-radiology\", \"split\": \"test\", \"max_samples\": 200},\n",
        "]\n",
        "\n",
        "datasets = {}\n",
        "for spec in dataset_specs:\n",
        "    ds = load_dataset(spec[\"name\"], split=spec[\"split\"])\n",
        "    if spec.get(\"max_samples\"):\n",
        "        ds = ds.select(range(spec[\"max_samples\"]))\n",
        "    ds = ds.with_format(\"python\")  # keep PIL images\n",
        "    datasets[spec[\"name\"]] = ds\n",
        "    print(f\"{spec['name']}[{spec['split']}] -> {len(ds)} samples\")\n",
        "\n",
        "# Keep one dataset handy for quick examples below\n",
        "dataset = datasets[dataset_specs[0][\"name\"]]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d755a764",
      "metadata": {
        "id": "d755a764"
      },
      "outputs": [],
      "source": [
        "sample_generate = dataset[0]\n",
        "image = sample_generate[\"image\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a0db40e",
      "metadata": {
        "id": "0a0db40e"
      },
      "outputs": [],
      "source": [
        "system_message = \"You are a digital radiologist who can understand the medical scan of images code the concepts and provide captions\"\n",
        "\n",
        "user_prompt = \"\"\"Create a description based on the provided image and return the description of the image with details of the scan as captions, the concepts and their descriptions, only the concepts that are extracted\"\"\"\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": system_message},\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": user_prompt},\n",
        "            {\"type\": \"image\", \"image\": image},\n",
        "        ],\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "610bd79b",
      "metadata": {
        "id": "610bd79b"
      },
      "outputs": [],
      "source": [
        "# Turn chat messages into a single string prompt\n",
        "chat_text = processor.apply_chat_template(\n",
        "    messages,\n",
        "    add_generation_prompt=True,   # <-- important for inference\n",
        "    tokenize=False,\n",
        ")\n",
        "\n",
        "# Build model inputs (batch size 1)\n",
        "inputs = processor(\n",
        "    text=[chat_text],\n",
        "    images=[image],\n",
        "    return_tensors=\"pt\",\n",
        "    padding=True,\n",
        ")\n",
        "\n",
        "# Move to correct device\n",
        "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "# Generate\n",
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=256,\n",
        "        do_sample=True,\n",
        "        temperature=0.9,\n",
        "        top_p=0.9,\n",
        "    )\n",
        "\n",
        "# Option 1: decode only the new tokens (without re-printing the prompt)\n",
        "gen_only_ids = generated_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
        "output_text = tokenizer.decode(gen_only_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"MODEL OUTPUT:\\n\")\n",
        "print(output_text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample_generate"
      ],
      "metadata": {
        "id": "Jv2fkTXWidtZ"
      },
      "id": "Jv2fkTXWidtZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7a16ee0e",
      "metadata": {
        "id": "7a16ee0e"
      },
      "source": [
        "## Batched scoring and saving to Google Drive\n",
        "\n",
        "These cells batch inference across multiple datasets and save JSONL outputs to Google Drive (or locally if Drive is not available).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bdbe2ed",
      "metadata": {
        "id": "0bdbe2ed"
      },
      "outputs": [],
      "source": [
        "from importlib import reload\n",
        "from pathlib import Path\n",
        "\n",
        "from src.llmft.inference import score as score_utils\n",
        "reload(score_utils)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf5778d",
      "metadata": {
        "id": "0cf5778d"
      },
      "outputs": [],
      "source": [
        "# Generation and batching settings\n",
        "batch_size = 4\n",
        "max_samples = None  # set to an int to cap samples per dataset\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=256,\n",
        "    do_sample=True,\n",
        "    temperature=0.9,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "# Where to save outputs (Google Drive if available)\n",
        "drive_output_dir = Path(\"/content/drive/MyDrive/llmft_scores/4Bit_Quant_Gemma_YL\")\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount(\"/content/drive\", force_remount=False)\n",
        "except Exception:\n",
        "    print(\"Colab drive not available; saving locally.\")\n",
        "    drive_output_dir = Path(\"scores\")\n",
        "\n",
        "drive_output_dir.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"Saving outputs to: {drive_output_dir}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "397b97a9",
      "metadata": {
        "id": "397b97a9"
      },
      "outputs": [],
      "source": [
        "results_paths = score_utils.score_multiple_and_save(\n",
        "    datasets=datasets,\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    system_message=system_message,\n",
        "    user_prompt=user_prompt,\n",
        "    output_dir=drive_output_dir,\n",
        "    batch_size=batch_size,\n",
        "    max_samples=max_samples,\n",
        "    gen_kwargs=gen_kwargs,\n",
        ")\n",
        "\n",
        "for name, path in results_paths.items():\n",
        "    print(f\"Saved {name} -> {path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git pull"
      ],
      "metadata": {
        "id": "UIDTFLVyjLk3"
      },
      "id": "UIDTFLVyjLk3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wa2FBj6EjkCE"
      },
      "id": "Wa2FBj6EjkCE",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}