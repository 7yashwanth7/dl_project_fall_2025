{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
    "\n",
    "# Install Gemma release branch from Hugging Face\n",
    "%pip install \"transformers>=4.51.3\"\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"datasets==3.3.2\" \\\n",
    "  \"accelerate==1.4.0\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.45.3\" \\\n",
    "  \"trl==0.15.2\" \\\n",
    "  \"peft==0.14.0\" \\\n",
    "  \"pillow==11.1.0\" \\\n",
    "  protobuf \\\n",
    "  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300d561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Make project src importable\n",
    "project_root = Path.cwd().parent\n",
    "sys.path.append(str(project_root / \"src\"))\n",
    "\n",
    "# Load environment variables for HF tokens if available\n",
    "try:\n",
    "    from dotenv import load_dotenv\n",
    "    load_dotenv(project_root / \".env\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "hf_token = os.getenv(\"HUGGINGFACE_TOKEN\") or os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "token_kwargs = {\"token\": hf_token} if hf_token else {}\n",
    "print(\"HF token loaded\" if hf_token else \"HF token not set; proceeding without one.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86645fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llmft.data_preprocessing.preprocess_utils import read_json, get_cui_mapping\n",
    "\n",
    "cui_json = read_json(project_root / \"mapping_files\" / \"cui_mapping.json\")\n",
    "cui_mapping = get_cui_mapping(cui_json)\n",
    "print(f\"Loaded {len(cui_mapping)} CUI mapping entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "base_model_id = \"google/gemma-3-4b-pt\"\n",
    "processor_id  = \"google/gemma-3-4b-it\"          # <-- IMPORTANT: same as training\n",
    "adapter_dir   = \"gemma-3-cui-finetuned-sample1\"\n",
    "\n",
    "# 1) Load processor (IT) and tokenizer\n",
    "processor = AutoProcessor.from_pretrained(processor_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# 2) Re-add CUI tokens exactly like during training\n",
    "#    (you used plain codes like \"C0041618\", not \"\")\n",
    "cui_tokens = list(cui_mapping.keys())\n",
    "num_added = tokenizer.add_tokens(cui_tokens)\n",
    "print(\"Added\", num_added, \"CUI tokens\")\n",
    "\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# 3) Load base model (PT) with 4-bit quant\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    base_model_id,\n",
    "    **model_kwargs,\n",
    ")\n",
    "\n",
    "# 4) Resize embeddings to match tokenizer (base vocab + CUI tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 5) Attach LoRA adapter you trained\n",
    "model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc555f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Configure the datasets to score; add more entries as needed\n",
    "# Example: {\"name\": \"dataset/name\", \"split\": \"validation\", \"max_samples\": 200}\n",
    "dataset_specs = [\n",
    "    {\"name\": \"eltorio/ROCOv2-radiology\", \"split\": \"train\", \"max_samples\": 200},\n",
    "]\n",
    "\n",
    "datasets = {}\n",
    "for spec in dataset_specs:\n",
    "    ds = load_dataset(spec[\"name\"], split=spec[\"split\"], **token_kwargs)\n",
    "    if spec.get(\"max_samples\"):\n",
    "        ds = ds.select(range(spec[\"max_samples\"]))\n",
    "    ds = ds.with_format(\"python\")  # keep PIL images\n",
    "    datasets[spec[\"name\"]] = ds\n",
    "    print(f\"{spec['name']}[{spec['split']}] -> {len(ds)} samples\")\n",
    "\n",
    "# Keep one dataset handy for quick examples below\n",
    "dataset = datasets[dataset_specs[0][\"name\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d755a764",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generate = dataset[0]\n",
    "image = sample_generate[\"image\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0db40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a digital radiologist who can understand the medical scan of images code the concepts and provide captions\"\n",
    "\n",
    "user_prompt = \"\"\"Create a description based on the provided image and return the description of the image with details of the scan as captions, the concepts and their descriptions, only the concepts that are extracted\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": system_message},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn chat messages into a single string prompt\n",
    "chat_text = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # <-- important for inference\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Build model inputs (batch size 1)\n",
    "inputs = processor(\n",
    "    text=[chat_text],\n",
    "    images=[image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# Move to correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "# Option 1: decode only the new tokens (without re-printing the prompt)\n",
    "gen_only_ids = generated_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "output_text = tokenizer.decode(gen_only_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"MODEL OUTPUT:\\n\")\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a16ee0e",
   "metadata": {},
   "source": [
    "## Batched scoring and saving to Google Drive\n",
    "\n",
    "These cells batch inference across multiple datasets and save JSONL outputs to Google Drive (or locally if Drive is not available).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdbe2ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "from pathlib import Path\n",
    "\n",
    "from llmft.inference import score as score_utils\n",
    "reload(score_utils)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf5778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generation and batching settings\n",
    "batch_size = 4\n",
    "max_samples = None  # set to an int to cap samples per dataset\n",
    "\n",
    "gen_kwargs = dict(\n",
    "    max_new_tokens=256,\n",
    "    do_sample=True,\n",
    "    temperature=0.9,\n",
    "    top_p=0.9,\n",
    ")\n",
    "\n",
    "# Where to save outputs (Google Drive if available)\n",
    "drive_output_dir = Path(\"/content/drive/MyDrive/llmft_scores\")\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "except Exception:\n",
    "    print(\"Colab drive not available; saving locally.\")\n",
    "    drive_output_dir = Path(\"scores\")\n",
    "\n",
    "drive_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Saving outputs to: {drive_output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b97a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_paths = score_utils.score_multiple_and_save(\n",
    "    datasets=datasets,\n",
    "    model=model,\n",
    "    processor=processor,\n",
    "    system_message=system_message,\n",
    "    user_prompt=user_prompt,\n",
    "    output_dir=drive_output_dir,\n",
    "    batch_size=batch_size,\n",
    "    max_samples=max_samples,\n",
    "    gen_kwargs=gen_kwargs,\n",
    ")\n",
    "\n",
    "for name, path in results_paths.items():\n",
    "    print(f\"Saved {name} -> {path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
