{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de3ca81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Pytorch & other libraries\n",
    "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
    "\n",
    "# Install Gemma release branch from Hugging Face\n",
    "%pip install \"transformers>=4.51.3\"\n",
    "\n",
    "# Install Hugging Face libraries\n",
    "%pip install  --upgrade \\\n",
    "  \"datasets==3.3.2\" \\\n",
    "  \"accelerate==1.4.0\" \\\n",
    "  \"evaluate==0.4.3\" \\\n",
    "  \"bitsandbytes==0.45.3\" \\\n",
    "  \"trl==0.15.2\" \\\n",
    "  \"peft==0.14.0\" \\\n",
    "  \"pillow==11.1.0\" \\\n",
    "  protobuf \\\n",
    "  sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b17dcaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
    "from peft import PeftModel\n",
    "from datasets import load_dataset\n",
    "\n",
    "base_model_id = \"google/gemma-3-4b-pt\"\n",
    "processor_id  = \"google/gemma-3-4b-it\"          # <-- IMPORTANT: same as training\n",
    "adapter_dir   = \"gemma-3-cui-finetuned-sample1\"\n",
    "\n",
    "# 1) Load processor (IT) and tokenizer\n",
    "processor = AutoProcessor.from_pretrained(processor_id)\n",
    "tokenizer = processor.tokenizer\n",
    "\n",
    "# 2) Re-add CUI tokens exactly like during training\n",
    "#    (you used plain codes like \"C0041618\", not \"\")\n",
    "cui_tokens = list(cui_mapping.keys())\n",
    "num_added = tokenizer.add_tokens(cui_tokens)\n",
    "print(\"Added\", num_added, \"CUI tokens\")\n",
    "\n",
    "processor.tokenizer = tokenizer\n",
    "\n",
    "# 3) Load base model (PT) with 4-bit quant\n",
    "model_kwargs = dict(\n",
    "    attn_implementation=\"eager\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_quant_storage=torch.bfloat16,\n",
    "    ),\n",
    ")\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    base_model_id,\n",
    "    **model_kwargs,\n",
    ")\n",
    "\n",
    "# 4) Resize embeddings to match tokenizer (base vocab + CUI tokens)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# 5) Attach LoRA adapter you trained\n",
    "model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f75aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_generate = dataset[1001]\n",
    "image = sample_generate[\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0db40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message = \"You are a digital radiologist who can understand the medical scan of images code the concepts and provide captions\"\n",
    "\n",
    "user_prompt = \"\"\"Create a description based on the provided image and return the description of the image with details of the scan as captions, the concepts and their descriptions, only the concepts that are extracted\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": system_message},\n",
    "        ],\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": user_prompt},\n",
    "            {\"type\": \"image\", \"image\": image},\n",
    "        ],\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn chat messages into a single string prompt\n",
    "chat_text = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,   # <-- important for inference\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "# Build model inputs (batch size 1)\n",
    "inputs = processor(\n",
    "    text=[chat_text],\n",
    "    images=[image],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")\n",
    "\n",
    "# Move to correct device\n",
    "inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Generate\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "# Option 1: decode only the new tokens (without re-printing the prompt)\n",
    "gen_only_ids = generated_ids[:, inputs[\"input_ids\"].shape[-1]:]\n",
    "output_text = tokenizer.decode(gen_only_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"MODEL OUTPUT:\\n\")\n",
    "print(output_text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
