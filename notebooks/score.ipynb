{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6de3ca81",
      "metadata": {
        "collapsed": true,
        "id": "6de3ca81"
      },
      "outputs": [],
      "source": [
        "# Install Pytorch & other libraries\n",
        "%pip install \"torch>=2.4.0\" tensorboard torchvision\n",
        "\n",
        "# Install Gemma release branch from Hugging Face\n",
        "%pip install \"transformers>=4.51.3\"\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \\\n",
        "  \"datasets==3.3.2\" \\\n",
        "  \"accelerate==1.4.0\" \\\n",
        "  \"evaluate==0.4.3\" \\\n",
        "  \"bitsandbytes==0.45.3\" \\\n",
        "  \"trl==0.15.2\" \\\n",
        "  \"peft==0.14.0\" \\\n",
        "  \"pillow==11.1.0\" \\\n",
        "  protobuf \\\n",
        "  sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount the notebook on to the google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# Set the working directory to dl_project_fall_2025\n",
        "import os\n",
        "os.chdir(\"/content/drive/MyDrive/DL_Project_2025/dl_project_fall_2025\")\n",
        "\n",
        "# Auto relaod doesnt work in google colab, so you can use reload to reload your function calls\n",
        "from importlib import reload"
      ],
      "metadata": {
        "id": "kDaNMKiLfvz3",
        "collapsed": true
      },
      "id": "kDaNMKiLfvz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing git token and huggig face tokens\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "\n",
        "# Retrieve the GitHub Token from Colab secrets\n",
        "GH_TOKEN = userdata.get('git_token') # Ensure you stored your PAT under the secret name 'GH_TOKEN'\n",
        "hf_token = userdata.get('hugging_face')\n",
        "login(hf_token)\n",
        "\n",
        "# Configure Git to use the PAT directly in the remote URL for the 'origin'\n",
        "!git remote set-url origin https://{GH_TOKEN}@github.com/7yashwanth7/dl_project_fall_2025.git\n",
        "!git config --global user.email \"7yashwanth7@gmail.com\" # Modify to your username and pwd\n",
        "!git config --global user.name \"7yashwanth7\""
      ],
      "metadata": {
        "id": "JVmACrOhfzJI"
      },
      "id": "JVmACrOhfzJI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e86645fe",
      "metadata": {
        "id": "e86645fe"
      },
      "outputs": [],
      "source": [
        "from src.llmft.data_preprocessing import preprocess_utils\n",
        "\n",
        "defaults = preprocess_utils.read_yaml('src/llmft/config/defaults.yaml')\n",
        "cui_mapping_json = preprocess_utils.read_json('mapping_files/cui_mapping.json')\n",
        "cui_mapping = preprocess_utils.get_cui_mapping(cui_mapping_json)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b17dcaa",
      "metadata": {
        "id": "6b17dcaa",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForImageTextToText, AutoProcessor, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from datasets import load_dataset\n",
        "\n",
        "base_model_id = \"google/gemma-3-4b-pt\"\n",
        "processor_id  = \"google/gemma-3-4b-it\"          # <-- IMPORTANT: same as training\n",
        "adapter_dir   = \"gemma-3-cui-finetuned-sample1\"\n",
        "\n",
        "# 1) Load processor (IT) and tokenizer\n",
        "processor = AutoProcessor.from_pretrained(processor_id)\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "tokenizer.padding_side = \"left\"\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# 2) Re-add CUI tokens exactly like during training\n",
        "#    (you used plain codes like \"C0041618\", not \"\")\n",
        "cui_tokens = list(cui_mapping.keys())\n",
        "num_added = tokenizer.add_tokens(cui_tokens)\n",
        "print(\"Added\", num_added, \"CUI tokens\")\n",
        "\n",
        "processor.tokenizer = tokenizer\n",
        "\n",
        "# 3) Load base model (PT) with 4-bit quant\n",
        "model_kwargs = dict(\n",
        "    attn_implementation=\"eager\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\",\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_storage=torch.bfloat16,\n",
        "    ),\n",
        ")\n",
        "\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    base_model_id,\n",
        "    **model_kwargs,\n",
        ")\n",
        "\n",
        "# 4) Resize embeddings to match tokenizer (base vocab + CUI tokens)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 5) Attach LoRA adapter you trained\n",
        "model = PeftModel.from_pretrained(model, adapter_dir)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fc555f3",
      "metadata": {
        "collapsed": true,
        "id": "2fc555f3"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "#9927 samples\n",
        "dataset_specs = [\n",
        "    {\"name\": \"eltorio/ROCOv2-radiology\", \"split\": \"test\", \"max_samples\": 100},\n",
        "]\n",
        "\n",
        "datasets = {}\n",
        "for spec in dataset_specs:\n",
        "    ds = load_dataset(spec[\"name\"], split=spec[\"split\"])\n",
        "    if spec.get(\"max_samples\"):\n",
        "        ds = ds.select(range(spec[\"max_samples\"]))\n",
        "    ds = ds.with_format(\"python\")  # keep PIL images\n",
        "    datasets[spec[\"name\"]] = ds\n",
        "    print(f\"{spec['name']}[{spec['split']}] -> {len(ds)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a16ee0e",
      "metadata": {
        "id": "7a16ee0e"
      },
      "source": [
        "## Batched scoring and saving to Google Drive\n",
        "\n",
        "These cells batch inference across multiple datasets and save JSONL outputs to Google Drive (or locally if Drive is not available).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "from typing import Any, Dict, Iterable, List, Optional, Sequence\n",
        "\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "system_message = \"You are a digital radiologist who can understand the medical scan of images code the concepts and provide captions\"\n",
        "user_prompt = \"\"\"Create a description based on the provided image and return the description of the image with details of the scan as captions, the concepts and their descriptions, only the concepts that are extracted\"\"\"\n",
        "\n",
        "def build_messages(sample, system_message, user_prompt):\n",
        "    return [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": [{\"type\": \"text\", \"text\": system_message}],\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                {\"type\": \"text\", \"text\": user_prompt},\n",
        "                {\"type\": \"image\"}\n",
        "            ],\n",
        "        },\n",
        "    ]\n",
        "\n",
        "\n",
        "def prepare_batch(samples, processor, system_message, user_prompt):\n",
        "    messages = [build_messages(s, system_message, user_prompt) for s in samples]\n",
        "\n",
        "    chat_texts = [\n",
        "        processor.apply_chat_template(m, add_generation_prompt=True, tokenize=False)\n",
        "        for m in messages\n",
        "    ]\n",
        "    images = [[s[\"image\"]] for s in samples]\n",
        "\n",
        "    # Optional sanity check\n",
        "    assert len(chat_texts) == len(images), (len(chat_texts), len(images))\n",
        "\n",
        "    inputs = processor(\n",
        "        text=chat_texts,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "        padding=True,\n",
        "    )\n",
        "    return inputs\n",
        "\n",
        "def decode_generations(\n",
        "    generated_ids: torch.Tensor,\n",
        "    prompt_input_ids: torch.Tensor,\n",
        "    tokenizer):\n",
        "\n",
        "    \"\"\"Decode only the generated tokens (strip the prompt).\"\"\"\n",
        "    prompt_len = prompt_input_ids.shape[1]\n",
        "    gen_only = generated_ids[:, prompt_len:]\n",
        "    return tokenizer.batch_decode(gen_only, skip_special_tokens=True)\n",
        "\n",
        "def score_dataset(\n",
        "    dataset: Any,\n",
        "    model,\n",
        "    processor,\n",
        "    system_message: str,\n",
        "    user_prompt: str,\n",
        "    batch_size: int = 4,\n",
        "    max_samples: Optional[int] = None,\n",
        "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    \"\"\"Generate outputs for a dataset in batches.\"\"\"\n",
        "    gen_kwargs = gen_kwargs or {}\n",
        "    total = len(dataset)\n",
        "    if max_samples is not None:\n",
        "        total = min(total, max_samples)\n",
        "\n",
        "    results: List[Dict[str, Any]] = []\n",
        "    model.eval()\n",
        "\n",
        "    for start in tqdm(range(0, total, batch_size), desc=\"Scoring\", leave=False):\n",
        "        end = min(start + batch_size, total)\n",
        "        batch = [dataset[i] for i in range(start, end)]\n",
        "\n",
        "        inputs = prepare_batch(batch, processor, system_message, user_prompt)\n",
        "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
        "\n",
        "        with torch.no_grad():\n",
        "            generated_ids = model.generate(**inputs, **gen_kwargs)\n",
        "\n",
        "        decoded = decode_generations(generated_ids, inputs[\"input_ids\"], processor.tokenizer)\n",
        "\n",
        "        for sample, output_text in zip(batch, decoded):\n",
        "            results.append(\n",
        "                {\n",
        "                    \"id\": sample.get(\"image_id\"),\n",
        "                    \"caption\": sample.get(\"caption\"),\n",
        "                    \"cui\": sample.get(\"cui\"),\n",
        "                    \"generation\": output_text.strip(),\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def save_jsonl(records: Iterable[Dict[str, Any]], path: Path) -> None:\n",
        "    \"\"\"Save a list of dicts to a JSONL file.\"\"\"\n",
        "    path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    with path.open(\"w\", encoding=\"utf-8\") as f:\n",
        "        for rec in records:\n",
        "            f.write(json.dumps(rec) + \"\\n\")\n",
        "\n",
        "\n",
        "def score_batch_dataset(\n",
        "    datasets: Dict[str, Any],\n",
        "    model,\n",
        "    processor,\n",
        "    system_message: str,\n",
        "    user_prompt: str,\n",
        "    batch_size: int = 4,\n",
        "    max_samples: Optional[int] = None,\n",
        "    gen_kwargs: Optional[Dict[str, Any]] = None,\n",
        ") -> Dict[str, Path]:\n",
        "    \"\"\"Score multiple datasets and save results per dataset.\"\"\"\n",
        "    paths: Dict[str, Path] = {}\n",
        "    for name, ds in datasets.items():\n",
        "        results = score_dataset(\n",
        "            dataset=ds,\n",
        "            model=model,\n",
        "            processor=processor,\n",
        "            system_message=system_message,\n",
        "            user_prompt=user_prompt,\n",
        "            batch_size=batch_size,\n",
        "            max_samples=max_samples,\n",
        "            gen_kwargs=gen_kwargs,\n",
        "        )\n",
        "        # save_jsonl(results, output_dir)\n",
        "        # paths[name] = output_dir\n",
        "    return results"
      ],
      "metadata": {
        "id": "zV2hXQs-hkmi"
      },
      "id": "zV2hXQs-hkmi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0cf5778d",
      "metadata": {
        "id": "0cf5778d"
      },
      "outputs": [],
      "source": [
        "# Generation and batching settings\n",
        "batch_size = 8\n",
        "max_samples = None  # set to an int to cap samples per dataset\n",
        "\n",
        "gen_kwargs = dict(\n",
        "    max_new_tokens=128,\n",
        "    do_sample=False\n",
        ")\n",
        "\n",
        "#Results\n",
        "results = score_batch_dataset(\n",
        "    datasets=datasets,\n",
        "    model=model,\n",
        "    processor=processor,\n",
        "    system_message=system_message,\n",
        "    user_prompt=user_prompt,\n",
        "    batch_size=batch_size,\n",
        "    max_samples=max_samples,\n",
        "    gen_kwargs=gen_kwargs,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Results"
      ],
      "metadata": {
        "id": "Hu36gZBkUK-a"
      },
      "id": "Hu36gZBkUK-a"
    },
    {
      "cell_type": "code",
      "source": [
        "Experiment_Name = \"4Bit_Qunat_Gemma_YL_test_trained_model_check\""
      ],
      "metadata": {
        "id": "VGXNi0paUOps"
      },
      "id": "VGXNi0paUOps",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Save results\n",
        "output_folder = f\"/content/drive/MyDrive/DL_Project_2025/Score_Results\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "output_path = os.path.join(output_folder, f\"{Experiment_Name}.jsonl\")\n",
        "\n",
        "# Dump it to Json\n",
        "with open(output_path, 'w', encoding='utf-8') as f:\n",
        "    for entry in results:\n",
        "        # json.dump converts the dictionary to a text string\n",
        "        f.write(json.dumps(entry) + \"\\n\")"
      ],
      "metadata": {
        "id": "J9hdk7CBvJDL"
      },
      "id": "J9hdk7CBvJDL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate Results"
      ],
      "metadata": {
        "id": "XxmH3CM4VAQI"
      },
      "id": "XxmH3CM4VAQI"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json(output_path, lines=True)"
      ],
      "metadata": {
        "id": "Oe7Q5t2kPDJz"
      },
      "id": "Oe7Q5t2kPDJz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Robust patterns\n",
        "CAPTION_RE = re.compile(\n",
        "    r\"Caption:\\s*(.*?)\\s*(?:\\nConcept descriptions:|\\nConcepts:|$)\",\n",
        "    flags=re.S\n",
        ")\n",
        "CONCEPTS_RE = re.compile(\n",
        "    r\"Concepts:\\s*(.*)\\s*$\",\n",
        "    flags=re.S\n",
        ")\n",
        "\n",
        "def extract_caption_and_concepts(text: str):\n",
        "    text = (text or \"\").strip()\n",
        "\n",
        "    # Caption\n",
        "    m_cap = CAPTION_RE.search(text)\n",
        "    caption = m_cap.group(1).strip() if m_cap else \"\"\n",
        "\n",
        "    # Concepts (as list)\n",
        "    m_con = CONCEPTS_RE.search(text)\n",
        "    concepts_str = m_con.group(1).strip() if m_con else \"\"\n",
        "\n",
        "    # split by comma if multiple concepts\n",
        "    concepts = [c.strip() for c in concepts_str.split(\",\") if c.strip()] if concepts_str else []\n",
        "\n",
        "    return pd.Series({\"caption_extracted\": caption, \"concepts_list\": concepts})"
      ],
      "metadata": {
        "id": "P26If8jlQk8p"
      },
      "id": "P26If8jlQk8p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[[\"caption_extracted\", \"concepts_extracted\"]] = df[\"generation\"].apply(extract_caption_and_concepts)"
      ],
      "metadata": {
        "id": "Msop7aG9PjIR"
      },
      "id": "Msop7aG9PjIR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1TR7vRQaVmJw"
      },
      "id": "1TR7vRQaVmJw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}