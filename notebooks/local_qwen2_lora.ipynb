{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LoRA/QLoRA fine-tune Qwen2-VL on local images + captions\n",
    "\n",
    "Use this notebook when you already have a folder of images and a CSV mapping `image_name.jpg` to caption text.\n",
    "\n",
    "What it does:\n",
    "- Loads train/val CSVs and image files from local paths\n",
    "- Sets up Qwen2-VL-2B-Instruct with 4-bit (QLoRA) to fit on a single Colab GPU\n",
    "- Runs a short training loop to verify loss decreases\n",
    "- Saves LoRA adapters\n",
    "\n",
    "Adjust paths and column names in the config cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install deps (pin to avoid ABI issues on Colab)\n",
    "!pip install -q --force-reinstall \\\n",
    "    torch==2.3.1 torchvision==0.18.1 torchaudio==2.3.1 \\\n",
    "    transformers==4.44.2 accelerate peft \\\n",
    "    numpy==1.26.4 pandas==2.2.2 pillow==10.3.0 datasets tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable TensorFlow/Flax imports to avoid pulling jax/tf built against different numpy\n",
    "import os\n",
    "os.environ['USE_TF'] = '0'\n",
    "os.environ['TRANSFORMERS_NO_TF'] = '1'\n",
    "os.environ['USE_FLAX'] = '0'\n",
    "os.environ['TRANSFORMERS_NO_FLAX'] = '1'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config: set paths and caption map (image_id -> text). Images are image_id + '.jpg' in IMAGE_DIR.\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_DIR = Path('/content/local_data')  # change to your root folder\n",
    "IMAGE_DIR = BASE_DIR / 'images'  # folder containing image_id.jpg\n",
    "\n",
    "# Provide your caption mapping here. Keys are image_ids (without .jpg), values are text.\n",
    "caption_map = {\n",
    "    # 'image_001': 'Your caption text here',\n",
    "    # 'image_002': 'Another caption',\n",
    "}\n",
    "\n",
    "# Split IDs into train/val lists. Replace with your own splits.\n",
    "TRAIN_IDS = list(caption_map.keys())[:20]\n",
    "VAL_IDS = list(caption_map.keys())[20:30]\n",
    "\n",
    "# Training params\n",
    "BATCH_SIZE = 2\n",
    "EPOCHS = 1\n",
    "LR = 2e-4\n",
    "MAX_STEPS = 30  # stop early for a quick loss check\n",
    "\n",
    "MODEL_ID = 'Qwen/Qwen2.5-VL-3B-Instruct'  # Qwen2.5 3B vision-language\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build datasets from dict + image folder\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, ids, caption_map, image_dir):\n",
    "        self.ids = list(ids)\n",
    "        self.caption_map = caption_map\n",
    "        self.image_dir = Path(image_dir)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.ids[idx]\n",
    "        img_path = self.image_dir / f\"{image_id}.jpg\"\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "        caption = self.caption_map[image_id]\n",
    "        return {'image': image, 'caption': caption}\n",
    "\n",
    "train_ds = ImageCaptionDataset(TRAIN_IDS, caption_map, IMAGE_DIR)\n",
    "val_ds = ImageCaptionDataset(VAL_IDS, caption_map, IMAGE_DIR)\n",
    "print('Train samples:', len(train_ds), 'Val samples:', len(val_ds))\n",
    "print('Example IDs:', TRAIN_IDS[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor and prompt builder\n",
    "from transformers import AutoProcessor\n",
    "\n",
    "# Qwen2-VL processor needs trust_remote_code to enable multimodal processing\n",
    "processor = AutoProcessor.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "\n",
    "def build_prompt(caption):\n",
    "    messages = [\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': [\n",
    "                {'type': 'image'},\n",
    "                {'type': 'text', 'text': 'Provide a concise caption for this image.'},\n",
    "            ],\n",
    "        }\n",
    "    ]\n",
    "    return processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True) + caption\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collate fn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [build_prompt(ex['caption']) for ex in batch]\n",
    "    images = [ex['image'] for ex in batch]\n",
    "    inputs = processor(text=texts, images=images, padding=True, return_tensors='pt')\n",
    "    labels = inputs['input_ids'].clone()\n",
    "    labels[inputs['attention_mask'] == 0] = -100\n",
    "    return {**inputs, 'labels': labels}\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print({k: (v.shape if torch.is_tensor(v) else type(v)) for k, v in batch.items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model without 4-bit (bf16/fp16), avoiding bitsandbytes issues on Python 3.12\n",
    "from transformers import AutoModelForImageTextToText\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto',\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj']\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=32,\n",
    "    target_modules=target_modules,\n",
    "    lora_dropout=0.05,\n",
    "    bias='none',\n",
    "    task_type='SEQ_2_SEQ_LM',\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop (short run to check loss decline)\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LR)\n",
    "total_steps = min(MAX_STEPS, EPOCHS * len(train_loader))\n",
    "warmup = max(1, int(0.03 * total_steps))\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, warmup, total_steps)\n",
    "\n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{EPOCHS}')\n",
    "    for batch in pbar:\n",
    "        if step >= total_steps:\n",
    "            break\n",
    "        batch = {k: v.to(model.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        pbar.set_postfix({'loss': round(loss.item(), 4)})\n",
    "        step += 1\n",
    "    if step >= total_steps:\n",
    "        break\n",
    "\n",
    "print('Finished steps:', step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple validation loss pass (optional)\n",
    "import torch.nn.functional as F\n",
    "\n",
    "model.eval()\n",
    "val_losses = []\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(val_loader, desc='Val'):\n",
    "        batch = {k: v.to(model.device) if torch.is_tensor(v) else v for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        val_losses.append(outputs.loss.item())\n",
    "avg_val = sum(val_losses) / len(val_losses)\n",
    "print('Val loss:', round(avg_val, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save adapters\n",
    "OUT_DIR = BASE_DIR / 'qwen2vl_lora_adapters'\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "model.save_pretrained(OUT_DIR)\n",
    "processor.save_pretrained(OUT_DIR)\n",
    "print('Saved adapters to', OUT_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick qualitative check\n",
    "model.eval()\n",
    "sample = next(iter(val_loader))\n",
    "sample = {k: v.to(model.device) if torch.is_tensor(v) else v for k, v in sample.items()}\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(**sample, max_new_tokens=64)\n",
    "    out_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "print('Generated:', out_text)\n",
    "print('Reference:', train_df.iloc[0][TEXT_COL])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}